# -*- coding: utf-8 -*-
"""Albarran_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vTc_jHQkJBMIGFTwWcTJHCggr7kZ2zuY
"""

import os
import numpy as np
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

from torch.utils.data import DataLoader, TensorDataset, Dataset
from transformers import AutoImageProcessor, AutoModelForImageClassification
# needed for class labels 0-9 else res-50 gives error
from datasets import load_dataset, concatenate_datasets, ClassLabel
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
from PIL import Image

#!rm -rf /content/directory # in case to delete files

# Step 2: Split into 70% train, 15% val, 20% test
ds = load_dataset("EnmmmmOvO/insect-pest-dataset")

train_ds = ds["train"]
val_ds   = ds["validation"]
test_ds  = ds["test"]

# Keep only first N images or randomly sample
train_ds = train_ds.shuffle(seed=42).select(range(15000))  # 15k images instead of 75k
val_ds  = val_ds.shuffle(seed=42).select(range(4000))
test_ds  = test_ds.shuffle(seed=42).select(range(4500))

print(train_ds)
print(val_ds)
print(test_ds)

num_classes = len(set(train_ds["label"]))
train_ds = train_ds.cast_column("label", ClassLabel(num_classes=num_classes))
val_ds = val_ds.cast_column("label", ClassLabel(num_classes=num_classes))
test_ds = test_ds.cast_column("label", ClassLabel(num_classes=num_classes))
print(num_classes)

processor = AutoImageProcessor.from_pretrained("google/efficientnet-b1", size={"height": 128, "width": 128})
model = AutoModelForImageClassification.from_pretrained("google/efficientnet-b1",num_labels=num_classes, ignore_mismatched_sizes=True)

# Training augmentation
train_transform = transforms.Compose([
    transforms.Resize((128,128)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
])

# Validation / Test (no augmentation)
val_transform = transforms.Resize((128,128))

def transform(batch, train=True):
    pixel_values = []
    for img in batch["image"]:
        # Convert to PIL Image if not already
        if not isinstance(img, Image.Image):
            img = Image.fromarray(np.array(img))

        # Convert grayscale or RGBA to RGB
        img = img.convert("RGB")

        # Apply augmentations / resize
        if train:
            img = train_transform(img)
        else:
            img = val_transform(img)

        # Convert to tensor via processor
        pv = processor(img, return_tensors="pt")["pixel_values"].squeeze(0)
        pixel_values.append(pv)
    batch["pixel_values"] = pixel_values
    return batch


train_ds = train_ds.with_transform(lambda b: transform(b, train=True))
val_ds   = val_ds.with_transform(lambda b: transform(b, train=False))
test_ds  = test_ds.with_transform(lambda b: transform(b, train=False))



# 4. DATALOADER COLLATOR
def collate_fn(batch):
    pixel_values = torch.stack([item["pixel_values"] for item in batch])
    labels = torch.tensor([item["label"] for item in batch])
    return pixel_values, labels

# 5. DATA LOADERS
batch_size = 32

train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4) #, weight_decay=1e-4

# Lists for plotting
train_loss_list = []
train_acc_list = []
val_loss_list = []
val_acc_list = []

epochs = 40 # cahnged as 50 was too much for computer to handle

for epoch in range(epochs):
    model.train()

    running_loss = 0.0
    correct = 0
    total = 0

    # -------- TRAINING PHASE --------
    for pixel_values, labels in train_loader:
        pixel_values = pixel_values.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        outputs = model(pixel_values=pixel_values)
        loss = criterion(outputs.logits, labels)

        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        _, predicted = torch.max(outputs.logits, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    avg_loss = running_loss / len(train_loader)
    accuracy = correct / total

    # Save results for plotting
    train_loss_list.append(avg_loss)
    train_acc_list.append(accuracy)



    # -------- VALIDATION PHASE --------
    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0

    with torch.no_grad():
        for pixel_values, labels in val_loader:
            pixel_values = pixel_values.to(device)
            labels = labels.to(device)

            outputs = model(pixel_values=pixel_values)
            loss = criterion(outputs.logits, labels)

            val_loss += loss.item()

            _, predicted = torch.max(outputs.logits, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    avg_val_loss = val_loss / len(val_loader)
    val_accuracy = val_correct / val_total

    # Save validation results
    val_loss_list.append(avg_val_loss)
    val_acc_list.append(val_accuracy)


    print(f"\nEpoch {epoch+1}/{epochs}")
    print(f"Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy*100:.2f}%")
    print(f"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy*100:.2f}%")

model.eval()  # set model to evaluation mode
correct = 0
total = 0
running_loss = 0.0
criterion = nn.CrossEntropyLoss()

with torch.no_grad():  # no gradients needed
    for pixel_values, labels in test_loader:
        pixel_values = pixel_values.to(device)
        labels = labels.to(device)

        outputs = model(pixel_values=pixel_values)
        loss = criterion(outputs.logits, labels)
        running_loss += loss.item()

        # get predictions
        #preds = torch.max(pred_class, 1)
        preds = outputs.logits.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

avg_loss = running_loss / len(test_loader)
accuracy = correct / total

print(f"Test Loss: {avg_loss:.4f} | Test Accuracy: {accuracy:.4f}")

epoch_plt = range(1, epochs + 1)

# Loss plot
plt.plot(epoch_plt, train_loss_list, label='Training Loss', linestyle='--', color='blue')
plt.plot(epoch_plt, val_loss_list, label='Validation Loss', linestyle='--', color='orange')
plt.title('Loss Graph')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# accuracy plot
plt.plot(epoch_plt, train_acc_list, label='Training Accuracy', linestyle='--', color='blue')
plt.plot(epoch_plt, val_acc_list, label='Validation Accuracy', color='green')
plt.title('Insect Classification Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# ---- CONFUSION MATRIX ----
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

def get_predictions(model, data_loader, device):
    model.eval()
    all_true = []
    all_pred = []

    with torch.no_grad():
        for pixel_values, labels in data_loader:
            pixel_values = pixel_values.to(device)
            labels = labels.to(device)

            outputs = model(pixel_values=pixel_values)
            preds = outputs.logits.argmax(dim=1)

            all_true.extend(labels.cpu().numpy())
            all_pred.extend(preds.cpu().numpy())
    return np.array(all_true), np.array(all_pred)

# Get predictions on validation or test set
true_labels, pred_labels = get_predictions(model, val_loader, device)
# Class names
class_names = train_ds.features["label"].names
# Confusion matrix
cm = confusion_matrix(true_labels, pred_labels)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap='Blues', values_format='d')
plt.title("EfficientNet-B1 Confusion Matrix")
plt.show()

# Limited Matrix
k = 15   # top 10 classes

# Find the k most common true labels
unique, counts = np.unique(true_labels, return_counts=True)
top_k_classes = unique[np.argsort(counts)[-k:]]

# Filter labels to only include those classes
mask = np.isin(true_labels, top_k_classes)
filtered_true = true_labels[mask] # limit classes
filtered_pred = pred_labels[mask]

filtered_cm = confusion_matrix(filtered_true, filtered_pred, labels=top_k_classes)

filtered_names = [class_names[i] for i in top_k_classes]

disp = ConfusionMatrixDisplay(confusion_matrix=filtered_cm, display_labels=filtered_names)
disp.plot(cmap='Blues', values_format='d')
plt.title("15 Common Classes")
plt.show()